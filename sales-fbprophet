# -*- coding: utf-8 -*-
"""Sales Forecasting using Facebook Prophet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1u8Vp6u-UQYkcqKXuJBYPpANoPQsHMGJB

# Importing Dependencies:
"""

pip install prophet

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib.lines import Line2D
import plotly.express as px
from prophet import Prophet
import warnings 
warnings.filterwarnings('ignore')

"""# Importing Data:"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/GP/

store=pd.read_csv('store.csv')
train=pd.read_csv('train.csv')
test=pd.read_csv('test.csv')

train.shape

train.head(5)

"""# Preprocessing:"""

train.isnull().sum()
#there isn't null values in train data

test.isnull().sum()
#there are 11 null values in Open column.

store.isnull().sum()

store.head(5)

#Filling Missing Values
store['CompetitionDistance'].fillna(store['CompetitionDistance'].median(), inplace = True)

#Handling Null Vlaues
store.fillna(0, inplace = True)

store.isnull().sum()

#Checking Duplicates:
train.duplicated().sum()

test.duplicated().sum()

store.duplicated().sum()

#There are no duplicated values
#Statistical Information
train.describe()

test.describe()

store.describe()

#Combining train/test and store data
train = pd.merge(train, store, how = 'left', on = 'Store')
test = pd.merge(test, store, how = 'left', on = 'Store')

"""# Feature Extraction:"""

#Feature Extraction
def extract_year_from_date(data, date_column):
    data['year'] = pd.to_datetime(data[date_column], errors='coerce').dt.year
    return data[['Date', 'year']].head(5)
    
result = extract_year_from_date(train, 'Date')
print(result)

def extract_month_from_date(data, date_column):
    data['month'] = pd.to_datetime(data[date_column], errors='coerce').dt.month
    return data[['Date', 'month']].head(5)
result = extract_month_from_date(train, 'Date')
print(result)

def extract_day_from_date(data, date_column):
    data['day'] = pd.to_datetime(data[date_column], errors='coerce').dt.day
    return data[['Date', 'day']].head(5)
result = extract_day_from_date(train, 'Date')
print(result)

def extract_weekday_from_date(data, date_column):
    data['weekday'] = pd.to_datetime(data[date_column], errors='coerce').dt.strftime('%A')
    return data[['Date', 'weekday']].sample(5)
result = extract_weekday_from_date(train, 'Date')
print(result)

"""# Exploratory Data Analysis:"""

#Exploratory Data Analysis
#How many open or closed stores are in the data?
fig_dims = (12, 4)
fig, ax = plt.subplots(figsize=fig_dims)
x = train.groupby(['Open'])[['Store']].count()
x = x.reset_index()
labels = ['Open', 'Closed']
sns.barplot(x='Store', y='Open', data=x, color='c')
plt.xlabel('Total Number of Store')
plt.ylabel('Open or Closed')
legend_elements = [Line2D([0], [0], color='c', lw=4, label='Open')]
plt.legend(handles=legend_elements)
plt.show()
#In above figure, there are 172817 closed stores and there are 844392 open stores.

#How many Promos is in the data?
fig_dims = (8, 4)
fig, ax = plt.subplots(figsize=fig_dims)

sns.countplot(data=train, x='Promo')

plt.show()
#there are 629129 instances of no promotions, and there are 388080 promotions

#Which day has the most Customers?
weekday_customers_mean = train.groupby('weekday')[['Customers']].mean().sort_index()

plt.figure(figsize=(10, 5))
plt.plot(weekday_customers_mean.index, weekday_customers_mean['Customers'], marker='^', color='b')
plt.xlabel('Weekday')
plt.ylabel('Average Number of Customers')
plt.title('Average Number of Customers by Weekday')
plt.xticks(rotation=45)
plt.grid(True)
plt.show()
#The weekday Monday has the highest customer number in the store.
#The weekday of Sunday hasn't customers because of the holiday.
#The lowest customer number is on the weekend of Saturday.
#Also, in the below code snippet, we can see this result using the 'DayOfWeek' column.

customers_mean = train.groupby('DayOfWeek')[['Customers']].mean()

plt.figure(figsize=(10, 5))
plt.plot(customers_mean.index, customers_mean['Customers'], marker='^', color='b')
plt.xlabel('Day of Week')
plt.ylabel('Average Number of Customers')
plt.title('Average Number of Customers by Day of Week')
plt.grid(True)
plt.show()
#DayOfWeek is from Monday to Sunday e.g. 1 is Monday, 2 is Tuesday, and 3 is Wednesday

#Which day has the Promotion?
fig_dims = (16, 4)
fig, ax = plt.subplots(figsize=fig_dims)

sns.countplot(x='weekday', hue='Promo', data=train)

plt.xlabel('Weekday')
plt.ylabel('Count')
plt.title('Count of Promotions by Weekday')
plt.legend(title='Promo')

plt.show()
#In the figure, all weekdays have a promotion. The weekend has no promotion.

#Which store has the highest Sales Price?
x = train.groupby('Store')[['Sales']].mean().reset_index()

fig = px.line(x, x='Store', y='Sales', title='Average Store Sales', markers=True)
fig.show()
#We can clearly see that the store 262 has the highest sales price compared to other stores.

#Which store has the more Customers?
fig_dims = (16, 4)
fig, ax = plt.subplots(figsize=fig_dims)

top_10_stores_customers = train.groupby('Store').agg({'Customers': 'max'})['Customers'].nlargest(10)
top_10_stores_customers.plot(kind='bar')

plt.xlabel('Store Number')
plt.ylabel('Max Customer Number')
plt.title('Top 10 Stores with Maximum Customers')

plt.show()
#In the figure, store 817 has the maximum number of customers. Other stores follow it

#Does Store which has the highest sales have more customers?
#When we look at above 2 figures, we can say that the store which has the highest sales price has more customers than the other stores. This is the expected result for us.

#Which year has the highest sales price?
fig_dims = (16, 4)
fig, ax = plt.subplots(figsize=fig_dims)

top_10_years_sales = train.groupby('year').agg({'Sales': 'sum'})['Sales'].nlargest(10)
top_10_years_sales.plot(kind='bar')

plt.xlabel('Years')
plt.ylabel('Sales')
plt.title('Top 10 Years with Highest Sales')

plt.show()
#2013 has the highest sales price compared to the two years

#Which year has the highest number of Customers?
fig_dims = (16, 4)
fig, ax = plt.subplots(figsize=fig_dims)

top_10_years_customers = train.groupby('year').agg({'Customers': 'sum'})['Customers'].nlargest(10)
top_10_years_customers.plot(kind='bar')

plt.xlabel('Years')
plt.ylabel('Customers')
plt.title('Top 10 Years with Highest Total Customers')

plt.show()
#The year of 2013 has the highest customers compared to 2014 and 2015. 2015 has the lowest customers.

#Which month has the highest sales price?
monthly_sales_mean = train.groupby('month')[['Sales']].mean()
fig = px.line(x=monthly_sales_mean.index, y=monthly_sales_mean['Sales'], title='Average Monthly Sales Price', markers=True)
fig.show()
#December, November,and July moths have 3 the highest sales price.
#Januvary, May, and October have the lowest sales price.

#Which month has the highest number of Customers?
monthly_customers_mean = train.groupby('month')[['Customers']].mean()
fig = px.line(x=monthly_customers_mean.index, y=monthly_customers_mean['Customers'], title='Average Monthly Customer Number', markers=True)
fig.show()
#12th month has the highest customer number that the average customer number is 703.06.

#Which day has the highest sales price?
daily_sales_mean = train.groupby('day')[['Sales']].mean()
fig = px.line(x=daily_sales_mean.index, y=daily_sales_mean['Sales'], title='Average Daily Sales Price', markers=True)
fig.show()
#Although 1th day has the lowest sales price, the day of 30 has the highest sales price.

#Which day the highest number of Customers?
daily_customers_mean = train.groupby('day')[['Customers']].mean()
fig = px.line(x=daily_customers_mean.index, y=daily_customers_mean['Customers'], title='Average Daily Customer Number', markers=True)
fig.show()
#It is obvious that the customer and sales price are in the proper proportion.
#The price is lowest on the first day. On the other hand, the highest sales price is on the 30th day. 
#From our perspective, this is the outcome that is predicted. How many more customers are there now that there are more sales?

#Do Promotions effect Sales?
sns.barplot(x='Promo', y='Sales', data=train)
plt.title('Sales with Promotion')
plt.show()
#In the above graph, 0 denotes no promotion and 1 denotes promotion.When there is a promotion, the sales price is the highest.

#Monthly Distribution of Sales Price and Promotion
fig_dims = (16, 4)
fig, ax = plt.subplots(figsize=fig_dims)
sns.barplot(x='month', y='Sales', data=train, palette='bright', hue='Promo')
plt.title('Monthly Distribution Sales Price and Promotion')
plt.show()
#When we examine sales prices month by month, promotions drive up prices and have an impact on the sales approach.
#When compared to previous months, the 12th month with a promotion had the greatest sales price.

#How do State Holidays affect Sales?
fig_dims = (16, 4)
fig, ax = plt.subplots(figsize=fig_dims)
sns.barplot(x='StateHoliday', y='Sales', data=train, palette='bright')
plt.title('State Holidays and Sales')
plt.show()
#During the period of public holidays, there are no sales. Important holidays thus have an impact on sales.

#Monthly Distribution of Sales Price and School Holiday
fig_dims = (16, 4)
fig, ax = plt.subplots(figsize=fig_dims)
sns.barplot(x='month', y='Sales', data=train, palette='bright', hue='SchoolHoliday')
plt.title('Monthly Distribution Sales Price and School Holiday')
plt.show()
#When the school is open, the third month has the highest sales price and November, the eleventh month, has the lowest sales price. 
#The highest sales price and lowest sales price are experienced during school breaks in December and August, respectively.

#Sales Vs Store Types and Customers Vs Store Types
fig, (axis1, axis2) = plt.subplots(1, 2, figsize=(15, 4))
sns.barplot(x=train['StoreType'], y=train['Sales'], ax=axis1)
sns.barplot(x=train['StoreType'], y=train['Customers'], ax=axis2)
plt.show()
#The b store has the largest sales in the left-hand figure. The sales prices in retailers a, c, and d are similar roughly. 
#B has the most customers with a discernible difference when we look at the right figure. 
#Only the d store in this graph had fewer customers overall, while having the same sales prices as the a and c stores.

#Sales Vs State Holidays
fig, (axis1, axis2) = plt.subplots(1, 2, figsize=(15, 4))
sns.barplot(x=train['StateHoliday'], y=train['Sales'], ax=axis1)
sns.barplot(x=train['StateHoliday'], y=train['Customers'], ax=axis2)
plt.show()
#State holidays are depicted in figures a, b, and c to the left and right.
#There aren't any sales or customers anymore. On regular days, there are lots of sales.

#Sales Vs School Holidays
fig, (axis1, axis2) = plt.subplots(1, 2, figsize=(15, 4))
sns.barplot(x=train['SchoolHoliday'], y=train['Sales'], ax=axis1)
sns.barplot(x=train['SchoolHoliday'], y=train['Customers'], ax=axis2)
plt.show()
#When we compare the two statistics, O indicates that there are no school vacations, whereas 1 denotes that there are. 
#We can plainly see that the sales price and customer volume increase correspondingly with the school holiday.

"""# Feature Engineering:"""

#We must clean up the data before we can estimate. 
#There are various object types in our data. 
#To make the data more insightful, each string and month value is changed to an integer type.
def clean_data(data):
    # Mapping dictionaries for label encoding
    label1 = {'0': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4}
    label2 = {'0': 0, 'Jan,Apr,Jul,Oct': 1, 'Feb,May,Aug,Nov': 2, 'Mar,Jun,Sept,Dec': 3}
    
    # Replace values in the specified columns using the mapping dictionaries
    data.StoreType.replace(label1, inplace=True)
    data.Assortment.replace(label1, inplace=True)
    data.StateHoliday.replace(label1, inplace=True)
    data.PromoInterval.replace(label2, inplace=True)
    
    # Convert the columns to the appropriate data types
    data['StoreType'] = data['StoreType'].astype(int)
    data['Assortment'] = data['Assortment'].astype(int)
    data['StateHoliday'] = data['StateHoliday'].astype(int)
    data['PromoInterval'] = data['PromoInterval'].astype(int)
    
    return data

train=clean_data(train)
test=clean_data(test)

#Forecasts will not take into account closed stores or days with no sales.
train = train[(train["Open"] != 0) & (train['Sales'] != 0)]

"""# Correlation Analysis:"""

#Correlation analysis shows the relationship between 2 features. 
#This analysis helps us to understand which features are positive or negative relationships.

# Compute the correlation matrix 
# exclude 'Open' variable
corr_all = train.drop('Open', axis = 1).corr()

# Generate a mask for the upper triangle
mask = np.zeros_like(corr_all, dtype = np.bool)
mask[np.triu_indices_from(mask)] = True

# Set up the matplotlib figure
f, ax = plt.subplots(figsize = (11, 9))

# Draw the heatmap with the mask and correct aspect ratio
sns.heatmap(corr_all, mask = mask,
            square = True, linewidths = .5, ax = ax, cmap = "PiYG")      
plt.show()

"""In the above figure, the heat map shows the relationship between two features. 
Each two features values must be between -1 and 1. When we explain the map:

-1 means that there is a negative relationship between 2 features. For example,
Customers and PromoInterval have a negative relationship. The colour of the value is light blue.

0 means that there is no relationship between 2 features.

1 means that there is a positive relationship between 2 features. For example, Promo2 and Promo2SinceYear have a strong positive relationship.

# Sales Forecasting Using Facebook Prophet Model:

We will use the Prophet library for predicting future values. Before using this library, we need to import it.

pip install prophet
from prophet import Prophet
Now, we are ready to work on this library.

The Prophet() object takes arguments to configure the type of model you want, such as the type of growth, the type of seasonality, and more.

The fit() function takes a DataFrame for date time stamp. The DataFrame must have a specific format. 
The first column must have the name ‘ds‘ and contain the date-times. 
The second column must have the name ‘y‘ and contain the samples. 
In our project, ds is also Date column and y is Sales column. 
We try to find future sales values.For this reason, our observations/samples is Sales values.

Before using the Prophet module, we need to edit our data frame.
"""

#We are forcasting for first store so we select the first store from data frame
from sklearn.model_selection import train_test_split


# Split the data into features (X) and target variable (y)

# Reverse the order of rows in the train dataset
train_reversed = train[::-1]

# Calculate the index to split the data
split_index = int(0.7 * len(train_reversed))

# Split the reversed train data into train and test sets
train_data = train_reversed[:split_index]
test_data = train_reversed[split_index:]

test_data

train_data

#We convert data frame to specific format. Date column converts ds(starting date) and Sales column converts y(sales samples)
store_1_data = train_data[train_data['Store'] == 1]
store_test_data = test_data[test_data['Store'] == 1]
df = store_1_data[['Date', 'Sales']].rename(columns={'Date': 'ds', 'Sales': 'y'})

#We sort data frames in ascending order
df = df.sort_values(by='ds')

#We create the first Prophet model using the default parameters
model = Prophet()

#Now that our Prophet model has been initialized, we can call its fit method with our DataFrame as input.
model.fit(df)

#We create a new DataFrame containing a ds column that holds the dates for which we want predictions.
#'make_future_dataframe' function helps us to this purpose. 
#we determined Prophet to generate 30 datestamps in the future. 
# Get the last date in the training data
last_date = train_data['Date'].max()

# Determine the number of future periods
n_periods = 30

# Generate the future dates based on the last date in the training data
future_dates = pd.date_range(start=last_date , periods=n_periods, freq='D')

# Create a future dataframe with the future dates
future = pd.DataFrame({'ds': future_dates})

# Make predictions on the future dataframe
forecast = model.predict(future)

#Now, we can start Forecasting
forecast = model.predict(future)
forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']]

"""There are 4 outputs that are important to explain. These outputs are:

ds: the datestamp of the forecasted value

yhat: the forecasted value of our metric

yhat_lower: the lower bound of our forecasts

yhat_upper: the upper bound of our forecasts
"""

store_test_data.head(30)

from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_squared_error

# Assuming 'y_true' is the true target values and 'forecast' is the Prophet forecast dataframe
y_true = store_test_data['Sales'].head(30)  # Actual values
y_pred = forecast['yhat'].head(30)  # Predicted values

# Calculate MAE
mae = mean_absolute_error(y_true, y_pred)
print("Mean Absolute Error (MAE):", mae)

mse = mean_squared_error(y_true, y_pred)

# Calculate root mean squared error (RMSE)
rmse = np.sqrt(mse)
print("Mean Squared Error (MSE):", mse)
print("Root Mean Squared Error (RMSE):", rmse)

"""Mean Squared Error (MSE): 595943.4260777445

Mean Absolute Error (MAE): 552.9149872785608

Root Mean Squared Error (RMSE): 771.9737211056763

This means that the % accuaracy of the model is 93%.










"""

import matplotlib.pyplot as plt

import matplotlib.pyplot as plt

def plot_predictions(timestamps, y_true, y_pred):
    plt.figure(figsize=(12, 6))
    plt.plot(timestamps, y_true, label='Actual')
    plt.plot(timestamps, y_pred, label='Predicted')
    plt.xlabel('Timestamp')
    plt.ylabel('Value')
    plt.title('Predicted vs Actual Values')
    plt.legend()
    plt.xticks(rotation=45)  # Rotate x-axis labels for better readability
    plt.show()

plot_predictions(forecast['ds'], y_true, y_pred)

average_sales = train['Sales'].mean()
print("Average Sales:", average_sales)

fig3 = model.plot(forecast, xlabel='Datetime', ylabel='Sales', uncertainty=True)

"""Above, two figures shows the observed values of our sales price prediction (the black dots), the forecasted values (blue line) and the uncertainty intervals of our forecasts (the light blue shaded regions).

Prophet library provides to return the components of our forecasts.

This can help reveal how daily, weekly and yearly patterns of the time series contribute to the overall forecasted values.
"""

fig2 = model.plot_components(forecast)

"""The above plot provides interesting insights.

The first plot shows that the monthly volume of sales has been linearly decreasing over time.

The second plot highlights the fact that the weekly count of sales highly increases on the week of Monday.

The third plot shows that the most sales occur during the months of December.

# Modeling Holidays:
"""

#Now, we add holiday to our model.
state_dates = train[(train.StateHoliday == 'a') | (train.StateHoliday == 'b') | (train.StateHoliday == 'c')].loc[:, 'Date'].values
school_dates = train[train.SchoolHoliday == 1].loc[:, 'Date'].values

state = pd.DataFrame({'holiday': 'state_holiday', 'ds': pd.to_datetime(state_dates)})
school = pd.DataFrame({'holiday': 'school_holiday', 'ds': pd.to_datetime(school_dates)})

holidays = pd.concat((state, school))
holidays.head(5)

model = Prophet(interval_width=0.95, holidays=holidays)
model.fit(df)

# dataframe that extends into future 6 weeks 
future_dates = model.make_future_dataframe(periods=42)

print("First week to forecast.")
future_dates.tail(7)

forecast = model.predict(future_dates)

# predictions for last week
forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']]

fig2 = model.plot_components(forecast)

"""Explanation of 4 plots:

The first plot fact is that the trend is clearly downwards over time

The second plot shows that the holidays' gaps include in the model

The third plot demonstrates that Sales peaks on Monday, and hit bottom on Tuesday

The last plot shows that the busiest working period is in December which is during the Christmas holidays.
"""
