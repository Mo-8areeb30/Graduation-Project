# -*- coding: utf-8 -*-
"""Sales Forecasting Using XGBoost.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uJhL_VWpgE-RCOibfG4e3djGmFTztNen

# Importing Dependencies:
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

import warnings
warnings.filterwarnings('ignore')

import datetime
import math
import calendar

def calculate_weights(y):
    weights = np.zeros(y.shape, dtype=float)
    indices = y != 0
    weights[indices] = 1. / (y[indices] ** 2)
    return weights

def calculate_rmspe(yhat, y):
    weights = calculate_weights(y)
    rmspe = np.sqrt(np.mean(weights * (y - yhat) ** 2))
    return rmspe

def calculate_rmspe_xg(yhat, y):
    y = y.get_label()
    y = np.exp(y) - 1
    yhat = np.exp(yhat) - 1
    weights = calculate_weights(y)
    rmspe = np.sqrt(np.mean(weights * (y - yhat) ** 2))
    return "rmspe", rmspe

"""# Importing Data:"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/GP/

store=pd.read_csv('store.csv')
train=pd.read_csv('train.csv')
test=pd.read_csv('test.csv')

train.shape, test.shape, store.shape

train.Store.nunique() == store.Store.nunique()

"""# Preprocessing:"""

df = train.merge(store, how='left', left_on=train.Store, right_on=store.Store)
df.drop(['key_0', 'Store_y'], axis=1, inplace=True)
df = df.rename(columns={'Store_x':'Store'})
df.shape

df_stats = df.describe().T.round(2)
df_stats
#There are in total 1115 stores with sales feature having 3849.93 volatility and feature customers having 464.41 volatility with a mean of 57773.82 and 633.15 respectively.

train_duplicates = train.duplicated().sum()
test_duplicates = test.duplicated().sum()

#there are no null values in train and 11 null values in test
train_total_nulls = train.isnull().sum().sum()
test_total_nulls = test.isnull().sum().sum()
train_total_nulls

print("Training data starts from: {}".format(train['Date'].min()))
print("Training data ends on: {}".format(train['Date'].max()))
print()

df['Date'] = pd.to_datetime(df['Date'])
df['Day'] = df['Date'].dt.day
df['Month'] = df['Date'].dt.month
df['Year'] = df['Date'].dt.year
#With my analysis, I hope to engineer and pre-process features while also exploring the dataset's seasonality and trend.

"""# Exploratory Data Analysis (EDA):"""

plt.figure(figsize=(18, 8))
plt.plot(df.groupby(df['Day']).sum()['Sales'])
plt.title("Sales vs. Day")
plt.xlabel('Day')
plt.ylabel('Sales')
plt.show()

"""The beginning of the month sees the majority of sales, with the end of the month seeing the least amount."""

plt.figure(figsize=(18, 8))
plt.plot(df.groupby(df['DayOfWeek']).sum()['Sales'])
plt.title("Sales vs. Day of Week")
plt.xlabel('Day of Week')
plt.ylabel('Sales')
plt.show()

"""The beginning of the week sees greater sales than the end."""

plt.figure(figsize=(18, 8))
plt.plot(df.groupby(df['Month']).sum()['Sales'])
plt.title("Sales vs. Month")
plt.xlabel('Month')
plt.ylabel('Sales')
plt.show()

"""By year's end, sales are relatively lower.


"""

zero_sales_count = df[df['Open'] == 0]['Sales'].value_counts()
zero_sales_count

"""172817 values are filled with 0. I will fill these values with np.NaN as some models like XGBoost can handle missing values and it might benefit from it."""

fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(18, 8))
cmap = sns.diverging_palette(230, 20, as_cmap=True)

corr_train = train.corr()
mask_train = np.triu(np.ones_like(corr_train, dtype=bool))
sns.heatmap(corr_train, mask=mask_train, cmap=cmap, annot=True, ax=ax1)
ax1.set_title('Train')

corr_test = test.corr()
mask_test = np.triu(np.ones_like(corr_test, dtype=bool))
sns.heatmap(corr_test, mask=mask_test, cmap=cmap, annot=True, ax=ax2)
ax2.set_title('Test')

plt.show()

"""Sales are highly correlated with feature Customers and feature Open and moderately correlated with Promo."""

plt.figure(figsize=(18, 8))
temp_df = df.sample(100000)
sns.scatterplot(x=temp_df['Sales'], y=temp_df['Customers'], hue=temp_df['Year'])
plt.title("Sales vs. Customers")
plt.show()

fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(18, 8))
temp_df = df.sample(100000)
sns.scatterplot(x=temp_df['Sales'], y=temp_df['Customers'], hue=df['Promo'], ax=ax1)
sns.scatterplot(x=temp_df['Sales'], y=temp_df['Customers'], hue=df['Promo2'], ax=ax2)
plt.show()

"""Promo1 appears to have had greater success for the shops! Let's look at the average sales for each promotion.

"""

promo1_mean_sales = df.groupby(df['Promo'])['Sales'].mean()[1]
promo2_mean_sales = df.groupby(df['Promo2'])['Sales'].mean()[1]
promo1_mean_sales > promo2_mean_sales

"""Promo 1 has a grater sales average

Let's find out if the type of store is a key element
"""

plt.figure(figsize=(18, 8))
temp_df = df.groupby(df['StoreType']).sum()
sns.barplot(x=temp_df.index, y=temp_df['Sales'], palette='Blues')
plt.title("Store Type vs Sales")
plt.xlabel('Store Type')
plt.ylabel('Sales')
plt.show()

"""Why does Store A outperform all other stores?"""

fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(18, 8))
temp_df = df.groupby(df['StoreType']).count()
sns.barplot(x=temp_df.index, y=temp_df['Promo'], ax=ax1, palette='Blues')
temp_df = df.groupby(df['StoreType']).mean()
sns.barplot(x=temp_df.index, y=temp_df['CompetitionDistance'], ax=ax2, palette='Blues')
plt.show()

"""Despite consistently placing second behind other stores in terms of competition distance (the distance in metres to the closest competitor store), Store A conducted the most promotions. So, it's fair to say that promotions are a significant problem, in my opinion. Seasonality, trends, and other elements may also be present. Let's examine the trend.

"""

from statsmodels.tsa.seasonal import seasonal_decompose

temp_df = train.copy()
temp_df['Date'] = pd.to_datetime(temp_df['Date'])
temp_df.set_index('Date', inplace=True)
temp_df['Sales'] = temp_df['Sales'].apply(lambda x: None if x == 0 else x)
temp_df['Sales'] = temp_df['Sales'].fillna(method='ffill').fillna(method='bfill')
temp_df = temp_df[['Sales']]
temp_df = temp_df.groupby(temp_df.index).sum()

result = seasonal_decompose(temp_df, model='additive', period=52)

fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(18, 8))
ax1.plot(result.trend)
ax1.axhline(y=temp_df['Sales'].mean(), color='r', linestyle='-', label='Sales Mean')
ax1.set_title("Trend")
ax2.plot(result.resid)
ax2.set_title("Error")
ax1.legend()
plt.show()

"""Since the trend line was above the average line by the end of 2014, 2015 has been a positive year. I wonder what caused the enormous surge at the start of 2014 and the sudden plummet at the end of 2014


"""

temp_df = df.copy()
temp_df.set_index('Date', inplace=True)
temp_df['Sales'] = temp_df['Sales'].apply(lambda x: None if x == 0 else x)
temp_df['Sales'] = temp_df['Sales'].fillna(method='ffill').fillna(method='bfill')
temp_df = temp_df.groupby(temp_df.index).mean()

fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(18, 8))
ax1.plot(temp_df['CompetitionDistance'], '.')
ax1.set_title("Date vs CompetitionDistance (on average)")
ax2.plot(temp_df['CompetitionOpenSinceMonth'], '.')
ax2.set_title("Date vs CompetitionOpenSinceMonth (on average)")
plt.show()

"""It appears that a new competitor entered the market towards the end of 2014, and since the distance was also considerably increased, it is possible that the competition moved, but these are only conjectures. The model might be beneficial in interpreting this conduct for the stores in the future.

If the trend had been declining, I wonder if the stores would have run fewer promotions. We'll see.

"""

plt.figure(figsize=(18, 8))
temp_df = df.copy()
temp_df.set_index('Date', inplace=True)
temp_df = temp_df[temp_df['Year'] == 2014]
temp_df = temp_df.groupby(temp_df['Month']).sum()
temp_df['Sales'] = temp_df['Sales'].apply(lambda x: None if x == 0 else x)
temp_df['Sales'] = temp_df['Sales'].fillna(method='ffill').fillna(method='bfill')

plt.title('Total Promos done in YEAR 2014')
sns.lineplot(x=temp_df.index, y=temp_df['Promo'], palette='Blues', label='Promo1')
sns.lineplot(x=temp_df.index, y=temp_df['Promo2'], palette='Blues', label='Promo2')
plt.legend()
plt.show()

"""I was right, Promos did significantly decrease during the last 3 months of 2014, which cause the sudden plummet in its trend.

# Feature Extraction:
"""

train['is_train'] = 1
test['is_train'] = 0
df = pd.concat([train, test])

features_x = ['Store', 'Date', 'DayOfWeek', 'Open', 'Promo', 'SchoolHoliday', 'StateHoliday']
features_y = ['SalesLog']

df.Date = pd.to_datetime(df.Date) #Converting date to required format
df = df.loc[~((df['Open'] == 1) & (df['Sales'] == 0))] #Removing rows with 0 Sales

df['StateHoliday'] = df['StateHoliday'].map({0: '0', 'a': 'a', 'b': 'b', 'c': 'c', '0': '0'})  # Convert mixed data types
df['StateHoliday'] = LabelEncoder().fit_transform(df['StateHoliday'])  # Encoding for XGBoost

var_name = 'Date'

df[var_name + 'Day'] = df[var_name].dt.day  # Adding day
df[var_name + 'Week'] = df[var_name].dt.week  # Adding week
df[var_name + 'Month'] = df[var_name].dt.month  # Adding month
df[var_name + 'Year'] = df[var_name].dt.year  # Adding year
df[var_name + 'DayOfYear'] = df[var_name].dt.dayofyear  # Adding dayofyear

features_x.remove(var_name)  # Removing Date
features_x.extend([var_name + 'Day', var_name + 'Week', var_name + 'Month', var_name + 'Year', var_name + 'DayOfYear'])

store['StoreType'] = LabelEncoder().fit_transform(store['StoreType'])  # Encoding StoreType
store['Assortment'] = LabelEncoder().fit_transform(store['Assortment'])  # Encoding Assortment
#This code performs label encoding on these columns to convert categorical values into numerical representations.

join_with = store['PromoInterval'].str.split(',', expand=True)
join_with.columns = join_with.columns.map(lambda x: str(x) + '_PromoInterval')
store = store.join(join_with, rsuffix='_PromoInterval')
#This code splits the values in that column based on commas, creates new columns with the split values, and joins them with the original DataFrame.

def monthToNum(value):
    if value == 'Sept':
        value = 'Sep'
    return list(calendar.month_abbr).index(value)

store['0_PromoInterval'] = store['0_PromoInterval'].map(lambda x: monthToNum(x) if str(x) != 'nan' else np.nan)
store['1_PromoInterval'] = store['1_PromoInterval'].map(lambda x: monthToNum(x) if str(x) != 'nan' else np.nan)
store['2_PromoInterval'] = store['2_PromoInterval'].map(lambda x: monthToNum(x) if str(x) != 'nan' else np.nan)
store['3_PromoInterval'] = store['3_PromoInterval'].map(lambda x: monthToNum(x) if str(x) != 'nan' else np.nan)
#This code maps the month abbreviations to month numbers in these columns.

competition_open = []
for index, value in store[['CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear']].iterrows():
    try:
        year, month = int(value['CompetitionOpenSinceYear']), int(value['CompetitionOpenSinceMonth'])
        date = pd.to_datetime(f"{year}-{month}-01", format='%Y-%m-%d')
        competition_open.append(date)
    except:
        competition_open.append(np.nan)
competition_open = pd.Series(competition_open)
competition_open.shape
#This code iterates over all Competition stores and adds them all into a single dataframe

store['CompetitionOpen'] = competition_open.astype(str).str.slice(stop=10)
#This code assigns the competition_open list to a new column called 'CompetitionOpen' in the store DataFrame. 
#It then converts the values in the 'CompetitionOpen' column to a string format using the strftime function with the format '%Y%m%d'.

promo = []
for index, value in store[['Promo2SinceWeek', 'Promo2SinceYear']].iterrows():
    try:
        year, week = int(value['Promo2SinceYear']), int(value['Promo2SinceWeek'])
        date = pd.to_datetime(f"{year}-W{week-1}")
        promo.append(date)
    except:
        promo.append(np.nan)
promo = pd.Series(promo)
promo.shape
#This code initializes an empty list called promo and then iterates over the rows of the 'Promo2SinceWeek' and 'Promo2SinceYear' columns in the store DataFrame.
#It tries to convert the values into a datetime format by combining the year and week values, and appends the resulting datetime to the promo list.
#If any error occurs during the conversion, it appends np.nan to the list. 
#Finally, it converts the promo list to a Pandas Series of datetime values and assigns it to the variable promo.

store['PromoSince'] = promo.astype(str).str.replace('-', '')
#This code you provided assigns the promo variable, which contains a series of datetime values, to the 'PromoSince' column in the store DataFrame. 
#It then converts the values in the 'PromoSince' column to a string format using the strftime function with the format '%Y%m%d'.

store_features = ['Store', 'StoreType', 'Assortment', 'CompetitionDistance', 'CompetitionOpen', 'PromoSince', '0_PromoInterval']
#initilaize relevant features

df = df.merge(store[store_features], how='left', on='Store')
#This code performs a left merge between the df DataFrame and the store DataFrame using the common column 'Store'. 
#The resulting merged DataFrame will contain the columns from both DataFrames.

features_x = list(set(features_x).union(store_features))
#This code combines the lists features_x and store_features, removing any duplicate elements, and assigns the result back to features_x.

df[features_x] = df[features_x].fillna(-999)
#This code fills missing values in each feature of the df DataFrame with the value -999, indicating that these values are out of range for the model.

df['CompetitionOpen'] = pd.to_datetime(df['CompetitionOpen'], errors='coerce').fillna(pd.to_datetime('1900-01-01')).dt.strftime('%Y%m%d').astype(int)
df['PromoSince'] = pd.to_datetime(df['PromoSince'], errors='coerce').fillna(pd.to_datetime('1900-01-01')).dt.strftime('%Y%m%d').astype(int)
#This code converts the 'Date' column of the df DataFrame to an integer representation, and also converts the 'CompetitionOpen' and 'PromoSince' columns to integers.

"""# Handling Outliers:"""

df['Zscore'] = (df['Sales'] - df['Sales'].mean()) / df['Sales'].std()
#In this code, we subtract the mean of the 'Sales' column from each data point and divide it by the standard deviation of the 'Sales' column. 
#The result is stored in the 'Zscore' column of the DataFrame 'df'.
#By calculating the Z-score, you can identify data points that are significantly above or below the mean, indicating unusual or outlier values.

thresh = 4.0

def check_outlier(value):
    if value >= thresh:
        return True
    else:
        return False

df['Outlier'] = df['Zscore'].apply(check_outlier)
#The code you provided defines a function called 'check_outlier' that takes a value as input and checks if it is greater than or equal to a threshold value ('thresh'). 
#If the value is greater than or equal to the threshold, the function returns True, indicating that it is an outlier.

store_data_sales = df.groupby([df['Store']])['Sales'].sum()
store_data_customers = df.groupby([df['Store']])['Customers'].sum()
store_data_open = df.groupby([df['Store']])['Open'].count()

store_data_sales_per_day = store_data_sales / store_data_open
store_data_customers_per_day = store_data_customers / store_data_open
store_data_sales_per_customer_per_day = store_data_sales_per_day / store_data_customers_per_day

df_store = pd.merge(store, store_data_sales_per_day.reset_index(name='SalesPerDay'), how='left', on=['Store'])
df_store = pd.merge(df_store, store_data_customers_per_day.reset_index(name='CustomersPerDay'), how='left', on=['Store'])
df_store = pd.merge(df_store, store_data_sales_per_customer_per_day.reset_index(name='SalesPerCustomersPerDay'), how='left', on=['Store'])

"""This code performs calculations based on the sales, customers, and store data. It calculates the following metrics per store:

store_data_sales: Total sales for each store.
store_data_customers: Total number of customers for each store.
store_data_open: Count of the "Open" column for each store, which represents the number of days the store was open.
Then, it calculates the following metrics per day for each store:

store_data_sales_per_day: Average sales per day for each store, calculated by dividing the total sales by the number of days the store was open.
store_data_customers_per_day: Average number of customers per day for each store, calculated by dividing the total number of customers by the number of days the store was open.
store_data_sales_per_customer_per_day: Average sales per customer per day for each store, calculated by dividing the average sales per day by the average number of customers per day.
Finally, it merges these calculated metrics with the original store data using the store identifier ('Store') as the common key.
"""

store_features = ['Store', 'SalesPerDay', 'CustomersPerDay', 'SalesPerCustomersPerDay']
features_x = list(set(features_x + store_features))
df = pd.merge(df, df_store[store_features], how='left', on=['Store'])
#After executing this code, the df DataFrame will have the additional columns 'SalesPerDay', 'CustomersPerDay', and 'SalesPerCustomersPerDay' 
#from the df_store DataFrame, joined based on the 'Store' column.

holidays_each_day_of_week = df.groupby(df.DayOfWeek).sum().StateHoliday
df = pd.merge(df, holidays_each_day_of_week.reset_index(name='HolidaysPerDayOfWeek'), on=['DayOfWeek'])
#After executing this code, the df DataFrame will have an additional column 'HolidaysPerDayOfWeek', 
#Which represents the sum of 'StateHoliday' for each day of the week, based on the 'DayOfWeek' column.

school_holidays_each_day_of_week = df.groupby(df.DayOfWeek).sum().SchoolHoliday
df = pd.merge(df, school_holidays_each_day_of_week.reset_index(name='SchoolHolidaysPerDayOfWeek'), on=['DayOfWeek'])
#After executing this code, the df DataFrame will have an additional column 'SchoolHolidaysPerDayOfWeek', 
#which represents the sum of 'SchoolHoliday' for each day of the week, based on the 'DayOfWeek' column.

promo_each_day_of_week = df.groupby(df.DayOfWeek).sum().Promo
df = pd.merge(df, promo_each_day_of_week.reset_index(name='PromoPerDayOfWeek'), on=['DayOfWeek'])
#After executing this code, the df DataFrame will have an additional column 'PromoPerDayOfWeek',
#Which represents the sum of 'Promo' for each day of the week, based on the 'DayOfWeek' column.

holidays_next_week = []
holidays_next_week_index = []

for index, value in df.groupby(df.Date).sum().iterrows():
    start_range = index + pd.DateOffset(days=7)
    end_range = index + pd.DateOffset(days=14)
    school_holidays = sum(df[(df.Date >= start_range) & (df.Date < end_range)].SchoolHoliday)
    state_holidays = sum(df[(df.Date >= start_range) & (df.Date < end_range)].StateHoliday)
    holidays_next_week.append(school_holidays + state_holidays)
    holidays_next_week_index.append(index)

holidays_next_week = pd.Series(holidays_next_week)
holidays_next_week.shape
#After executing this code, the holidays_next_week list will contain the sums of school holidays and state holidays for each date within the next week. 
#The holidays_next_week_index list will store the corresponding index values. 
#Finally, a pandas Series is created from holidays_next_week, representing the summed values, and its shape is returned.

holidays_this_week = []
index_list = []

for index, value in df.groupby(df.Date).sum().iterrows():
    start_range = index
    end_range = index + pd.DateOffset(days=7)
    school_holidays = sum(df[(df.Date >= start_range) & (df.Date < end_range)].SchoolHoliday)
    state_holidays = sum(df[(df.Date >= start_range) & (df.Date < end_range)].StateHoliday)
    holidays_this_week.append(school_holidays + state_holidays)
    index_list.append(index)

holidays_this_week = pd.Series(holidays_this_week)
holidays_this_week.shape
#After executing this code, the holidays_this_week list will contain the sums of school holidays and state holidays for each date within the current week.
#The index_list list will store the corresponding index values. 
#Finally, a pandas Series is created from holidays_this_week, representing the summed values, and its shape is returned.

holidays_last_week = []
holidays_last_week_index = []

for index, value in df.groupby(df.Date).sum().iterrows():
    start_range = index - pd.DateOffset(days=7)
    end_range = index - pd.DateOffset(days=1)
    school_holidays = sum(df[(df.Date >= start_range) & (df.Date <= end_range)].SchoolHoliday)
    state_holidays = sum(df[(df.Date >= start_range) & (df.Date <= end_range)].StateHoliday)
    holidays_last_week.append(school_holidays + state_holidays)
    holidays_last_week_index.append(index)

holidays_last_week = pd.Series(holidays_last_week)
holidays_last_week.shape
#In this code, the start and end range for the previous week are calculated by subtracting pd.DateOffset(days=7) and pd.DateOffset(days=1) from the index, respectively. 
#The sum function is then used to calculate the sum of 'SchoolHoliday' and 'StateHoliday' values within the specified range, 
#and the results are appended to the holidays_last_week list. The corresponding index values are stored in the holidays_last_week_index list. 
#Finally, a pandas Series is created from holidays_last_week, representing the summed values, and its shape is returned.

temp_df = pd.DataFrame({'HolidaysNextWeek': holidays_next_week, 'Date': holidays_next_week_index})
df = pd.merge(df, temp_df, on=['Date'])
#This will add the 'HolidaysNextWeek' column from temp_df to the df DataFrame, matching the rows based on the 'Date' column. 
#Now, the df DataFrame will have the 'HolidaysNextWeek' column containing the holidays count for the next week.

temp_df = pd.DataFrame({'HolidaysThisWeek': holidays_this_week, 'Date': index_list})
df = pd.merge(df, temp_df, on=['Date'])
#This will add the 'HolidaysThisWeek' column from temp_df to the df DataFrame, matching the rows based on the 'Date' column. 
#Now, the df DataFrame will have the 'HolidaysThisWeek' column containing the holidays count for the current week.

temp_df = pd.DataFrame({'HolidaysLastWeek': holidays_last_week, 'Date': holidays_next_week_index})
df = pd.merge(df, temp_df, on=['Date'])
#This will create a DataFrame temp_df with the columns 'HolidaysLastWeek' and 'Date' using holidays_last_week and holidays_next_week_index, respectively. 
#Then, it merges temp_df with the df DataFrame on the 'Date' column.

holidays_features = ['HolidaysPerDayOfWeek', 'SchoolHolidaysPerDayOfWeek', 'PromoPerDayOfWeek',
                     'HolidaysNextWeek', 'HolidaysThisWeek', 'HolidaysLastWeek']

features_x.extend(holidays_features)
features_x = list(set(features_x))
#In this code, I first define the holidays_features list containing the desired holiday-related features. 
#Then, I use the extend method to add the elements of holidays_features to the features_x list. 
#Finally, I convert features_x to a set to remove any duplicate elements and then convert it back to a list to assign it to features_x.

df['DaysTillMaxPromo'] = df.DayOfWeek.apply(lambda x: 4 - x)
#This code uses the apply method on the 'DayOfWeek' column to subtract the value of each day from 4, representing the number of days until the maximum promo day (4). 
#The result is assigned to the 'DaysTillMaxPromo' column in the DataFrame.

promo_features = ['DaysTillMaxPromo', 'PromoTomorrow', 'PromoYesterday']
features_x = list(set(features_x + promo_features))
#This code adds the elements in promo_features to features_x using the + operator. 
#The set function is then used to remove any duplicate elements
#Finally, the result is converted back to a list. Now features_x contains all the original features plus the promo-related features.

df.Sales = df.Sales.apply(lambda x: np.nan if x == 0 else x)
#This code uses the apply method on the 'Sales' column and applies a lambda function to each element. 
#If the element is equal to 0, it is replaced with np.nan (NaN); otherwise, it remains unchanged. 
#After executing this code, all the zero values in the 'Sales' column will be replaced with NaN values.

df.loc[df['is_train'] == 1, 'SalesLog'] = np.log1p(df.loc[df['is_train'] == 1, 'Sales'])
#This code calculates the natural logarithm of 1 plus the 'Sales' values for the rows where 'is_train' is equal to 1 using the np.log1p function. 
#The result is assigned to the 'SalesLog' column for those rows. The np.log1p function is used instead of np.log to handle zero values correctly.

len(features_x)

df.shape

null_count = df.isnull().sum().sum()
null_count

"""# Modelling:"""

import xgboost as xgb

df['PromoTomorrow'] = df.groupby('Store')['Promo'].shift(-1)
df['PromoYesterday'] = df.groupby('Store')['Promo'].shift(1)

# Filter the dataframe based on specified conditions to get the training data
data = df.loc[(df['is_train'] == 1) & (df['Open'] == 1) & (df['Outlier'] == False)]

# Split the data into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(data[features_x], data[features_y], test_size=0.1, random_state=42)

# Convert the training and testing data to DMatrix format for XGBoost
dtrain = xgb.DMatrix(x_train, label=y_train)
dtest = xgb.DMatrix(x_test, label=y_test)

# Set the parameters for XGBoost model
params = {
    'max_depth': 9,
    'eta': 0.01,
    'subsample': 0.75,
    'colsample_bytree': 0.6,
    'objective': 'reg:squarederror'
}

# Specify the number of boosting rounds
num_rounds = 5000

# Prepare the evaluation set for monitoring performance during training
evals = [(dtrain, 'train'), (dtest, 'test')]

# Train the XGBoost model
model = xgb.train(params, dtrain, num_rounds, evals,
                  feval=calculate_rmspe_xg, verbose_eval=250, early_stopping_rounds=250)

# Save the model
model.save_model('/content/drive/MyDrive/GP/XGBoost.xgb')

import matplotlib.pyplot as plt

predictions = model.predict(dtest)


# Plotting the predicted values in red and actual values in black
plt.scatter(predictions, y_test, color='red', label='Predicted', alpha=0.7)
plt.scatter(y_test, y_test, color='black', label='Actual', alpha=0.7)

plt.xlabel('Predicted Values')
plt.ylabel('Actual Values')
plt.title('Predicted vs Actual Values')
plt.legend()
plt.show()
